{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
    "\n",
    "### Module submission group\n",
    "- Group member 1\n",
    "    - Name: Kasonde Chewe\n",
    "    - Email: kc3745@drexel.edu\n",
    "- Group member 2\n",
    "    - Name: Ghanath V\n",
    "    - Email: gv347@drexel.edu\n",
    "- Group member 3\n",
    "    - Name: Kholoud Hamed M Al Nazzawi\n",
    "    - Email: ka974@drexel.edu\n",
    "- Group member 4\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "\n",
    "### Additional submission comments\n",
    "- Tutoring support received: NA\n",
    "- Other (other): NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Group 1\n",
    "\n",
    "## Module D _(14 points)_\n",
    "\n",
    "__D1.__ _(2 points)_ In this module, you will be working on a Jupyter Notebook as the input data. In Module A of Assignment Group 1, we worked on the script of the [Seinfeld Chronicles dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles). Due to a mistake, we have printed the scripts 50 times. As a result, the generated output is very large and it does not allow us to open the notebook. If we cannot open the notebook, we won't be able to recover the code we've written before the problematic line of code. As a fix, in this module we are going to recover a Jupyter Notebook by removing the undesired `outputs` and save the notebook with a new name. Therefore, the first thing to know is the format of a Jupyter Notebook. As you noticed, the Jupyter Notebooks' names end with `.ipynb`. But, what is the format of a Jupyter Notebook as a data type, .e.g., CSV, XML, HTML, etc.? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON\n"
     ]
    }
   ],
   "source": [
    "# D1:Inline(2/2)\n",
    "\n",
    "# What is '.ipynb' a type of, as a data serialization? \n",
    "# Your answer should be a common (uppercase) acronym, like HTML.\n",
    "print(\"JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D2.__ _(3 points)_ Now, if you've figured out the `data` format of Jupyter Notebooks, load the `data.ipynb` file located in the `data` directory using an appropriate module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D2:Function(3/3)\n",
    "\n",
    "import json\n",
    "\n",
    "import json\n",
    "\n",
    "def load_notebook_data(path_to_notebook):\n",
    "    \n",
    "    with open(path_to_notebook, \"r\", encoding=\"utf-8\") as f:\n",
    "        notebook_data = json.load(f)\n",
    "          \n",
    "    return notebook_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "(['cells', 'metadata', 'nbformat', 'nbformat_minor'],\n",
    " [{'cell_type': 'markdown',\n",
    "   'metadata': {},\n",
    "   'source': ['__A3.__ _(5 points)_ Use the `csv` module to read the contents of the `data/scripts.csv` file into a list. Examine this list. How many unique speaking characters are present in the scripts in total?\\n',\n",
    "    '\\n',\n",
    "    \"__Important__: please don't get stuck on cleaning text in this module! It's great to take note of issues in data and even address them, but the regular expressions (regex) required to get heavily into that work is ahead in __Chapter 4__ and so not required here. Please just count characters and words as best possible using the topics from Chapters 0&ndash;2 (naïvely, even), and utilize the markdown response boxes to discuss what you see as being the challenges in working with these data and what solutions might be.\"]},\n",
    "  {'cell_type': 'code',\n",
    "   'execution_count': 2,\n",
    "   'metadata': {},\n",
    "   'outputs': [{'name': 'stdout',\n",
    "     'output_type': 'stream',\n",
    "     'text': ['54617\\n']}],\n",
    "   'source': ['import csv\\n',\n",
    "    '\\n',\n",
    "    'reader = csv.reader(open(\"data/scripts.csv\", \"r\"))\\n',\n",
    "    'seinfeld = list(reader)\\n',\n",
    "    'print(len(seinfeld))']}])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cells', 'metadata', 'nbformat', 'nbformat_minor'],\n",
       " [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['__A3.__ _(5 points)_ Use the `csv` module to read the contents of the `data/scripts.csv` file into a list. Examine this list. How many unique speaking characters are present in the scripts in total?\\n',\n",
       "    '\\n',\n",
       "    \"__Important__: please don't get stuck on cleaning text in this module! It's great to take note of issues in data and even address them, but the regular expressions (regex) required to get heavily into that work is ahead in __Chapter 4__ and so not required here. Please just count characters and words as best possible using the topics from Chapters 0&ndash;2 (naïvely, even), and utilize the markdown response boxes to discuss what you see as being the challenges in working with these data and what solutions might be.\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 2,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'name': 'stdout',\n",
       "     'output_type': 'stream',\n",
       "     'text': ['54617\\n']}],\n",
       "   'source': ['import csv\\n',\n",
       "    '\\n',\n",
       "    'reader = csv.reader(open(\"data/scripts.csv\", \"r\"))\\n',\n",
       "    'seinfeld = list(reader)\\n',\n",
       "    'print(len(seinfeld))']}])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D2:SanityCheck\n",
    "\n",
    "notebook_data = load_notebook_data('data/data.ipynb')\n",
    "list(notebook_data.keys()), notebook_data['cells'][4:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D3.__ _(3 points)_ In Jupyter, cells come in varieties, such as `'markdown'` and `'code'`. We write our code in the cells of type `'code'` and explanations in the `'markdown'` cells. Here, you must complete the function to clean out a `cell`, given that the cell is of type `'code'`. Note: here, cleaning means removing the entire output, however you must retain the original schema of the `.ipynb` file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D3:Function(3/3)\n",
    "\n",
    "def clean_cell_outputs(cell):\n",
    "    \n",
    "    #---your code starts here---\n",
    "    if cell['cell_type'] == 'code':\n",
    "        cell['outputs'] = []\n",
    "    #---your code stops here---\n",
    "    \n",
    "    return cell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "({'cell_type': 'code',\n",
    "  'execution_count': 2,\n",
    "  'metadata': {},\n",
    "  'outputs': [{'name': 'stdout',\n",
    "    'output_type': 'stream',\n",
    "    'text': ['54617\\n']}],\n",
    "  'source': ['import csv\\n',\n",
    "   '\\n',\n",
    "   'reader = csv.reader(open(\"data/scripts.csv\", \"r\"))\\n',\n",
    "   'seinfeld = list(reader)\\n',\n",
    "   'print(len(seinfeld))']},\n",
    " {'cell_type': 'code',\n",
    "  'execution_count': 2,\n",
    "  'metadata': {},\n",
    "  'outputs': [],\n",
    "  'source': ['import csv\\n',\n",
    "   '\\n',\n",
    "   'reader = csv.reader(open(\"data/scripts.csv\", \"r\"))\\n',\n",
    "   'seinfeld = list(reader)\\n',\n",
    "   'print(len(seinfeld))']})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cell_type': 'code',\n",
       "  'execution_count': 2,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stdout',\n",
       "    'output_type': 'stream',\n",
       "    'text': ['54617\\n']}],\n",
       "  'source': ['import csv\\n',\n",
       "   '\\n',\n",
       "   'reader = csv.reader(open(\"data/scripts.csv\", \"r\"))\\n',\n",
       "   'seinfeld = list(reader)\\n',\n",
       "   'print(len(seinfeld))']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 2,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['import csv\\n',\n",
       "   '\\n',\n",
       "   'reader = csv.reader(open(\"data/scripts.csv\", \"r\"))\\n',\n",
       "   'seinfeld = list(reader)\\n',\n",
       "   'print(len(seinfeld))']})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D3:SanityCheck\n",
    "\n",
    "dirty_notebook_data = load_notebook_data('data/data.ipynb')\n",
    "notebook_data['cells'][5], clean_cell_outputs(dirty_notebook_data['cells'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D4.__ _(4 points)_ Finally, complete the function below, replacing any problematic `outputs` by using the clean_cell_outputs function. Note: at minimum, you must at minimum clean out any problematic outputs, but you don't _have_ to clean all of the outputs, i.e., removing the `'outputs'` of non-problematic `'code'` cells in the Jupyter Notebook you've loaded can be re-created when the notebook is run again later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D4:Function(4/4)\n",
    "    \n",
    "def process_all_cells(notebook_data):\n",
    "    cleaned_notebook_data = dict(notebook_data)\n",
    "    \n",
    "    #---your code starts here---\n",
    "    cleaned_cells = []\n",
    "\n",
    "    for cell in notebook_data[\"cells\"]:\n",
    "        cleaned_cell = clean_cell_outputs(cell)\n",
    "        cleaned_cells.append(cleaned_cell)\n",
    "\n",
    "    cleaned_notebook_data[\"cells\"] = cleaned_cells\n",
    "    #---your code stops here---\n",
    "                \n",
    "    return cleaned_notebook_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your output should be:\n",
    "```\n",
    "(['cells', 'metadata', 'nbformat', 'nbformat_minor'],\n",
    " [{'cell_type': 'markdown',\n",
    "   'metadata': {},\n",
    "   'source': ['__A3.__ _(5 points)_ Use the `csv` module to read the contents of the `data/scripts.csv` file into a list. Examine this list. How many unique speaking characters are present in the scripts in total?\\n',\n",
    "    '\\n',\n",
    "    \"__Important__: please don't get stuck on cleaning text in this module! It's great to take note of issues in data and even address them, but the regular expressions (regex) required to get heavily into that work is ahead in __Chapter 4__ and so not required here. Please just count characters and words as best possible using the topics from Chapters 0&ndash;2 (naïvely, even), and utilize the markdown response boxes to discuss what you see as being the challenges in working with these data and what solutions might be.\"]},\n",
    "  {'cell_type': 'code',\n",
    "   'execution_count': 2,\n",
    "   'metadata': {},\n",
    "   'outputs': [],\n",
    "   'source': ['import csv\\n',\n",
    "    '\\n',\n",
    "    'reader = csv.reader(open(\"data/scripts.csv\", \"r\"))\\n',\n",
    "    'seinfeld = list(reader)\\n',\n",
    "    'print(len(seinfeld))']}])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cells', 'metadata', 'nbformat', 'nbformat_minor'],\n",
       " [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['__A3.__ _(5 points)_ Use the `csv` module to read the contents of the `data/scripts.csv` file into a list. Examine this list. How many unique speaking characters are present in the scripts in total?\\n',\n",
       "    '\\n',\n",
       "    \"__Important__: please don't get stuck on cleaning text in this module! It's great to take note of issues in data and even address them, but the regular expressions (regex) required to get heavily into that work is ahead in __Chapter 4__ and so not required here. Please just count characters and words as best possible using the topics from Chapters 0&ndash;2 (naïvely, even), and utilize the markdown response boxes to discuss what you see as being the challenges in working with these data and what solutions might be.\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 2,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['import csv\\n',\n",
       "    '\\n',\n",
       "    'reader = csv.reader(open(\"data/scripts.csv\", \"r\"))\\n',\n",
       "    'seinfeld = list(reader)\\n',\n",
       "    'print(len(seinfeld))']}])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D4:SanityCheck\n",
    "\n",
    "cleaned_notebook_data = process_all_cells(notebook_data)\n",
    "list(cleaned_notebook_data.keys()), cleaned_notebook_data['cells'][4:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D5.__ _(2 points)_ Finally, complete the code in the below cell to save your Jupyter Notebook with the new name `'data/out.ipynb'`.\n",
    "\n",
    "For reference, your output could be:\n",
    "```\n",
    "(['cells', 'metadata', 'nbformat', 'nbformat_minor'],\n",
    " [{'cell_type': 'markdown',\n",
    "   'metadata': {},\n",
    "   'source': ['__A3.__ _(5 points)_ Use the `csv` module to read the contents of the `data/scripts.csv` file into a list. Examine this list. How many unique speaking characters are present in the scripts in total?\\n',\n",
    "    '\\n',\n",
    "    \"__Important__: please don't get stuck on cleaning text in this module! It's great to take note of issues in data and even address them, but the regular expressions (regex) required to get heavily into that work is ahead in __Chapter 4__ and so not required here. Please just count characters and words as best possible using the topics from Chapters 0&ndash;2 (naïvely, even), and utilize the markdown response boxes to discuss what you see as being the challenges in working with these data and what solutions might be.\"]},\n",
    "  {'cell_type': 'code',\n",
    "   'execution_count': 2,\n",
    "   'metadata': {},\n",
    "   'outputs': [],\n",
    "   'source': ['import csv\\n',\n",
    "    '\\n',\n",
    "    'reader = csv.reader(open(\"data/scripts.csv\", \"r\"))\\n',\n",
    "    'seinfeld = list(reader)\\n',\n",
    "    'print(len(seinfeld))']}])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cells', 'metadata', 'nbformat', 'nbformat_minor'],\n",
       " [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['__A3.__ _(5 points)_ Use the `csv` module to read the contents of the `data/scripts.csv` file into a list. Examine this list. How many unique speaking characters are present in the scripts in total?\\n',\n",
       "    '\\n',\n",
       "    \"__Important__: please don't get stuck on cleaning text in this module! It's great to take note of issues in data and even address them, but the regular expressions (regex) required to get heavily into that work is ahead in __Chapter 4__ and so not required here. Please just count characters and words as best possible using the topics from Chapters 0&ndash;2 (naïvely, even), and utilize the markdown response boxes to discuss what you see as being the challenges in working with these data and what solutions might be.\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 2,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['import csv\\n',\n",
       "    '\\n',\n",
       "    'reader = csv.reader(open(\"data/scripts.csv\", \"r\"))\\n',\n",
       "    'seinfeld = list(reader)\\n',\n",
       "    'print(len(seinfeld))']}])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D5:Inline(2/2)\n",
    "\n",
    "clean_file_path = 'data/out.ipynb'\n",
    "\n",
    "#---your code starts here---\n",
    "original_notebook_data = load_notebook_data(\"data/data.ipynb\")\n",
    "cleaned_notebook_data = process_all_cells(original_notebook_data)\n",
    "\n",
    "with open(clean_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_notebook_data, f)\n",
    "#---your code stops here---\n",
    "\n",
    "# Load the cleaned notebook to verify the result\n",
    "clean_notebook_data = load_notebook_data(clean_file_path)\n",
    "list(clean_notebook_data.keys()), clean_notebook_data['cells'][4:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
